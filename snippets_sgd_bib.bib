% TODO: Improve citation
@article{zhang2016,
  author    = {Chiyuan Zhang and
               Samy Bengio and
               Moritz Hardt and
               Benjamin Recht and
               Oriol Vinyals},
  title     = {Understanding deep learning requires rethinking generalization},
  journal   = {CoRR},
  volume    = {abs/1611.03530},
  year      = {2016}
}

@inproceedings{
zhang2018three,
title={Three Mechanisms of Weight Decay Regularization},
author={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},
booktitle={International Conference on Learning Representations},
year={2019},
% url={https://openreview.net/forum?id=B1lz-3Rct7},
}

@article{carbonnelle2018,
  author    = {Simon Carbonnelle and
               Christophe De Vleeschouwer},
  title     = {On layer-level control of {DNN} training and its impact on generalization},
  journal   = {CoRR},
  volume    = {abs/1806.01603},
  year      = {2018}
}

@incollection{Wu2018,
title = {How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective},
author = {Wu, Lei and Ma, Chao and E, Weinan},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8279--8288},
year = {2018},
publisher = {Curran Associates, Inc.}
}


% TODO: AAAI citation
@article{fort2018,
  author    = {Stanislav Fort and
               Adam Scherlis},
  title     = {The Goldilocks zone: Towards better understanding of neural network
               loss landscapes},
  journal   = {CoRR},
  volume    = {abs/1807.02581},
  year      = {2018}
}

@article{you2017,
  author    = {Yang You and
               Igor Gitman and
               Boris Ginsburg},
  title     = {Scaling {SGD} Batch Size to 32K for ImageNet Training},
  journal   = {CoRR},
  volume    = {abs/1708.03888},
  year      = {2017}
}

@book{martens2016second,
  title={Second-order optimization for neural networks},
  author={Martens, James},
  year={2016},
  publisher={University of Toronto (Canada)}
}



@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    % note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@ARTICLE{ari2018,
       author = {{Gur-Ari}, Guy and {Roberts}, Daniel A. and {Dyer}, Ethan},
        title = "{Gradient Descent Happens in a Tiny Subspace}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
         year = "2018",
        month = "Dec",
          eid = {arXiv:1812.04754},
        pages = {arXiv:1812.04754},
archivePrefix = {arXiv},
       eprint = {1812.04754},
 primaryClass = {cs.LG},
    %   adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181204754G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{yao2018,
  author    = {Zhewei Yao and
               Amir Gholami and
               Qi Lei and
               Kurt Keutzer and
               Michael W. Mahoney},
  title     = {Hessian-based Analysis of Large Batch Training and Robustness to Adversaries},
  journal   = {CoRR},
  volume    = {abs/1802.08241},
  year      = {2018}
}
maintained by Schloss Dagstuhl LZI, founded at University of Trier	homebrowsesearchabout


@article{martens2015,
  author    = {James Martens and
               Roger B. Grosse},
  title     = {Optimizing Neural Networks with Kronecker-factored Approximate Curvature},
  journal   = {CoRR},
  volume    = {abs/1503.05671},
  year      = {2015}
}

@ARTICLE{zhu2018,
       author = {{Zhu}, Zhanxing and {Wu}, Jingfeng and {Yu}, Bing and {Wu}, Lei and
         {Ma}, Jinwen},
        title = "{The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Minima and Regularization Effects}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2018",
        month = "Feb",
          eid = {arXiv:1803.00195},
        pages = {arXiv:1803.00195},
archivePrefix = {arXiv},
       eprint = {1803.00195},
 primaryClass = {stat.ML},
    %   adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180300195Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{jastrzebski2017,
  author    = {Stanislaw Jastrzebski and
               Zachary Kenton and
               Devansh Arpit and
               Nicolas Ballas and
               Asja Fischer and
               Yoshua Bengio and
               Amos J. Storkey},
  title     = {Three Factors Influencing Minima in {SGD}},
  journal   = {CoRR},
  volume    = {abs/1711.04623},
  year      = {2017}
}

@ARTICLE{xing2018,
       author = {{Xing}, Chen and {Arpit}, Devansh and {Tsirigotis}, Christos and
         {Bengio}, Yoshua},
        title = "{A Walk with SGD}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2018",
        month = "Feb",
          eid = {arXiv:1802.08770},
        pages = {arXiv:1802.08770},
archivePrefix = {arXiv},
       eprint = {1802.08770},
 primaryClass = {stat.ML},
    %   adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180208770X},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{goodfellow2014,
       author = {{Goodfellow}, Ian J. and {Vinyals}, Oriol and {Saxe}, Andrew M.},
        title = "{Qualitatively characterizing neural network optimization problems}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2014",
        month = "Dec",
          eid = {arXiv:1412.6544},
        pages = {arXiv:1412.6544},
archivePrefix = {arXiv},
       eprint = {1412.6544},
 primaryClass = {cs.NE},
    %   adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6544G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

% TODO: Improve citation
@article{goyal2017,
  author    = {Priya Goyal and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick and
               Pieter Noordhuis and
               Lukasz Wesolowski and
               Aapo Kyrola and
               Andrew Tulloch and
               Yangqing Jia and
               Kaiming He},
  title     = {Accurate, Large Minibatch {SGD:} Training ImageNet in 1 Hour},
  journal   = {CoRR},
  volume    = {abs/1706.02677},
  year      = {2017}
}

@ARTICLE{kingma2014,
       author = {{Kingma}, Diederik P. and {Ba}, Jimmy},
        title = "{Adam: A Method for Stochastic Optimization}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2014",
        month = "Dec",
          eid = {arXiv:1412.6980},
        pages = {arXiv:1412.6980},
archivePrefix = {arXiv},
       eprint = {1412.6980},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{masters2018,
  author    = {Dominic Masters and
               Carlo Luschi},
  title     = {Revisiting Small Batch Training for Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1804.07612},
  year      = {2018}
}

@ARTICLE{hoffer2017,
       author = {{Hoffer}, Elad and {Hubara}, Itay and {Soudry}, Daniel},
        title = "{Train longer, generalize better: closing the generalization gap in large batch training of neural networks}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2017",
        month = "May",
          eid = {arXiv:1705.08741},
        pages = {arXiv:1705.08741},
archivePrefix = {arXiv},
       eprint = {1705.08741},
 primaryClass = {stat.ML},
    %   adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170508741H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{alain2019,
  author    = {Guillaume Alain and
               Nicolas Le Roux and
               Pierre{-}Antoine Manzagol},
  title     = {Negative eigenvalues of the Hessian in deep neural networks},
  journal   = {CoRR},
  volume    = {abs/1902.02366},
  year      = {2019}
}


@article{sagun2017,
  author    = {Levent Sagun and
               Utku Evci and
               V. Ugur G{\"{u}}ney and
               Yann Dauphin and
               L{\'{e}}on Bottou},
  title     = {Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1706.04454},
  year      = {2017}
}


@InProceedings{dinh2017,
  title = 	 {Sharp Minima Can Generalize For Deep Nets},
  author = 	 {Laurent Dinh and Razvan Pascanu and Samy Bengio and Yoshua Bengio},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1019--1028},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/dinh17b/dinh17b.pdf},
%   url = 	 {http://proceedings.mlr.press/v70/dinh17b.html},
  abstract = 	 {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g.\ Hochreiter \& Schmidhuber (1997); Keskar et al.\ (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Or, depending on the definition of flatness, it is the same for any given minimum. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.}
}

@article{papayan2019,
  author    = {Vardan Papyan},
  title     = {Measurements of Three-Level Hierarchical Structure in the Outliers
               in the Spectrum of Deepnet Hessians},
  journal   = {CoRR},
  volume    = {abs/1901.08244},
  year      = {2019}
}

@ARTICLE{novak2018,
       author = {{Novak}, Roman and {Bahri}, Yasaman and {Abolafia}, Daniel A. and
         {Pennington}, Jeffrey and {Sohl-Dickstein}, Jascha},
        title = "{Sensitivity and Generalization in Neural Networks: an Empirical Study}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = "2018",
        month = "Feb",
          eid = {arXiv:1802.08760},
        pages = {arXiv:1802.08760},
archivePrefix = {arXiv},
       eprint = {1802.08760},
 primaryClass = {stat.ML},
    %   adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180208760N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



% TODO: Improve citation
@ARTICLE{jastrzebski2018,
       author = {{Jastrzebski}, Stanislaw and {Kenton}, Zachary and
         {Ballas}, Nicolas and {Fischer}, Asja and {Bengio}, Yoshua and
         {Storkey}, Amos},
        title = "{On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2018",
        month = "Jul",
          eid = {arXiv:1807.05031},
        pages = {arXiv:1807.05031},
archivePrefix = {arXiv},
       eprint = {1807.05031},
 primaryClass = {stat.ML},
    %   adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180705031J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{soatto2017,
  author    = {Alessandro Achille and
               Matteo Rovere and
               Stefano Soatto},
  title     = {Critical Learning Periods in Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1711.08856},
  year      = {2017}
}

@ARTICLE{arpit2017,
       author = {{Arpit}, Devansh and {Jastrz{\k{e}}bski}, Stanis{\l}aw and
         {Ballas}, Nicolas and {Krueger}, David and {Bengio}, Emmanuel and
         {Kanwal}, Maxinder S. and {Maharaj}, Tegan and {Fischer}, Asja and
         {Courville}, Aaron and {Bengio}, Yoshua and {Lacoste-Julien}, Simon},
        title = "{A Closer Look at Memorization in Deep Networks}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2017",
        month = "Jun",
          eid = {arXiv:1706.05394},
        pages = {arXiv:1706.05394},
archivePrefix = {arXiv},
       eprint = {1706.05394},
 primaryClass = {stat.ML},
    %   adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170605394A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{li2018,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6389--6399},
  year={2018}
}

@article{choromanska2015,
title = "The loss surfaces of multilayer networks",
abstract = "We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large-and small-size networks where for the latter poor quality local minima have nonzero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.",
author = "Anna Choromanska and Mikael Henaff and Michael Mathieu and {Ben Arous}, Gerard and Yann LeCun",
year = "2015",
language = "English (US)",
volume = "38",
pages = "192--204",
journal = "Journal of Machine Learning Research",
issn = "1532-4435",
publisher = "Microtome Publishing",
}

