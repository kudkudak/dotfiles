
@article{zhang_understanding_2016,
	title = {Understanding deep learning requires rethinking generalization},
	volume = {abs/1611.03530},
	journal = {CoRR},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2016}
}

@inproceedings{zhang_three_2019,
	title = {Three {Mechanisms} of {Weight} {Decay} {Regularization}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger},
	year = {2019}
}

@article{carbonnelle_layer-level_2018,
	title = {On layer-level control of {DNN} training and its impact on generalization},
	volume = {abs/1806.01603},
	journal = {CoRR},
	author = {Carbonnelle, Simon and Vleeschouwer, Christophe De},
	year = {2018}
}

@incollection{wu_how_2018,
	title = {How {SGD} {Selects} the {Global} {Minima} in {Over}-parameterized {Learning}: {A} {Dynamical} {Stability} {Perspective}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Lei and Ma, Chao and E, Weinan},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {8279--8288}
}

@article{fort_goldilocks_2018,
	title = {The {Goldilocks} zone: {Towards} better understanding of neural network loss landscapes},
	volume = {abs/1807.02581},
	journal = {CoRR},
	author = {Fort, Stanislav and Scherlis, Adam},
	year = {2018}
}

@article{you_scaling_2017,
	title = {Scaling {SGD} {Batch} {Size} to {32K} for {ImageNet} {Training}},
	volume = {abs/1708.03888},
	journal = {CoRR},
	author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	year = {2017}
}

@book{martens_second-order_2016,
	title = {Second-order optimization for neural networks},
	publisher = {University of Toronto (Canada)},
	author = {Martens, James},
	year = {2016}
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016}
}

@article{gur-ari_gradient_2018,
	title = {Gradient {Descent} {Happens} in a {Tiny} {Subspace}},
	journal = {arXiv e-prints},
	author = {Gur-Ari, Guy and Roberts, Daniel A. and Dyer, Ethan},
	month = dec,
	year = {2018},
	note = {\_eprint: 1812.04754},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {arXiv:1812.04754}
}

@article{yao_hessian-based_2018,
	title = {Hessian-based {Analysis} of {Large} {Batch} {Training} and {Robustness} to {Adversaries}},
	volume = {abs/1802.08241},
	journal = {CoRR},
	author = {Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2018}
}

@article{martens_optimizing_2015,
	title = {Optimizing {Neural} {Networks} with {Kronecker}-factored {Approximate} {Curvature}},
	volume = {abs/1503.05671},
	journal = {CoRR},
	author = {Martens, James and Grosse, Roger B.},
	year = {2015}
}

@article{zhu_anisotropic_2018,
	title = {The {Anisotropic} {Noise} in {Stochastic} {Gradient} {Descent}: {Its} {Behavior} of {Escaping} from {Minima} and {Regularization} {Effects}},
	journal = {arXiv e-prints},
	author = {Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
	month = feb,
	year = {2018},
	note = {\_eprint: 1803.00195},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {arXiv:1803.00195}
}

@article{jastrzebski_three_2017,
	title = {Three {Factors} {Influencing} {Minima} in {SGD}},
	volume = {abs/1711.04623},
	journal = {CoRR},
	author = {Jastrzebski, Stanislaw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos J.},
	year = {2017}
}

@article{xing_walk_2018,
	title = {A {Walk} with {SGD}},
	journal = {arXiv e-prints},
	author = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
	month = feb,
	year = {2018},
	note = {\_eprint: 1802.08770},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {arXiv:1802.08770}
}

@article{goodfellow_qualitatively_2014,
	title = {Qualitatively characterizing neural network optimization problems},
	journal = {arXiv e-prints},
	author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
	month = dec,
	year = {2014},
	note = {\_eprint: 1412.6544},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {arXiv:1412.6544}
}

@article{goyal_accurate_2017,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	volume = {abs/1706.02677},
	journal = {CoRR},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross B. and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	year = {2017}
}

@article{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	journal = {arXiv e-prints},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {\_eprint: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	pages = {arXiv:1412.6980}
}

@article{masters_revisiting_2018,
	title = {Revisiting {Small} {Batch} {Training} for {Deep} {Neural} {Networks}},
	volume = {abs/1804.07612},
	journal = {CoRR},
	author = {Masters, Dominic and Luschi, Carlo},
	year = {2018}
}

@article{hoffer_train_2017,
	title = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	journal = {arXiv e-prints},
	author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	month = may,
	year = {2017},
	note = {\_eprint: 1705.08741},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {arXiv:1705.08741}
}

@article{alain_negative_2019,
	title = {Negative eigenvalues of the {Hessian} in deep neural networks},
	volume = {abs/1902.02366},
	journal = {CoRR},
	author = {Alain, Guillaume and Roux, Nicolas Le and Manzagol, Pierre-Antoine},
	year = {2019}
}

@article{sagun_empirical_2017,
	title = {Empirical {Analysis} of the {Hessian} of {Over}-{Parametrized} {Neural} {Networks}},
	volume = {abs/1706.04454},
	journal = {CoRR},
	author = {Sagun, Levent and Evci, Utku and Güney, V. Ugur and Dauphin, Yann and Bottou, Léon},
	year = {2017}
}

@inproceedings{dinh_sharp_2017,
	address = {International Convention Centre, Sydney, Australia},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Sharp {Minima} {Can} {Generalize} {For} {Deep} {Nets}},
	volume = {70},
	abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g.{\textbackslash} Hochreiter \& Schmidhuber (1997); Keskar et al.{\textbackslash} (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Or, depending on the definition of flatness, it is the same for any given minimum. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	pages = {1019--1028}
}

@article{papyan_measurements_2019,
	title = {Measurements of {Three}-{Level} {Hierarchical} {Structure} in the {Outliers} in the {Spectrum} of {Deepnet} {Hessians}},
	volume = {abs/1901.08244},
	journal = {CoRR},
	author = {Papyan, Vardan},
	year = {2019}
}

@article{novak_sensitivity_2018,
	title = {Sensitivity and {Generalization} in {Neural} {Networks}: an {Empirical} {Study}},
	journal = {arXiv e-prints},
	author = {Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	month = feb,
	year = {2018},
	note = {\_eprint: 1802.08760},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {arXiv:1802.08760}
}

@article{jastrzebski_relation_2018,
	title = {On the {Relation} {Between} the {Sharpest} {Directions} of {DNN} {Loss} and the {SGD} {Step} {Length}},
	journal = {arXiv e-prints},
	author = {Jastrzebski, Stanislaw and Kenton, Zachary and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
	month = jul,
	year = {2018},
	note = {\_eprint: 1807.05031},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {arXiv:1807.05031}
}

@article{keskar_large-batch_2016,
	title = {On large-batch training for deep learning: {Generalization} gap and sharp minima},
	journal = {arXiv preprint arXiv:1609.04836},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	year = {2016}
}

@article{achille_critical_2017,
	title = {Critical {Learning} {Periods} in {Deep} {Neural} {Networks}},
	volume = {abs/1711.08856},
	journal = {CoRR},
	author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
	year = {2017}
}

@article{arpit_closer_2017,
	title = {A {Closer} {Look} at {Memorization} in {Deep} {Networks}},
	journal = {arXiv e-prints},
	author = {Arpit, Devansh and Jastrzębski, Stanis{\textbackslash}law and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
	month = jun,
	year = {2017},
	note = {\_eprint: 1706.05394},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {arXiv:1706.05394}
}

@inproceedings{li_visualizing_2018,
	title = {Visualizing the loss landscape of neural nets},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	year = {2018},
	pages = {6389--6399}
}

@article{choromanska_loss_2015,
	title = {The loss surfaces of multilayer networks},
	volume = {38},
	issn = {1532-4435},
	abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large-and small-size networks where for the latter poor quality local minima have nonzero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
	language = {English (US)},
	journal = {Journal of Machine Learning Research},
	author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Ben Arous, Gerard and LeCun, Yann},
	year = {2015},
	note = {Publisher: Microtome Publishing},
	pages = {192--204}
}

@article{zhang_understanding_2016-1,
	title = {Understanding deep learning requires rethinking generalization},
	volume = {abs/1611.03530},
	journal = {CoRR},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2016}
}

@article{fort_emergent_2019,
	title = {Emergent properties of the local geometry of neural loss landscapes},
	journal = {arXiv preprint arXiv:1910.05929},
	author = {Fort, Stanislav and Ganguli, Surya},
	year = {2019},
	note = {\_eprint: 1910.05929}
}

@article{you_scaling_2017-1,
	title = {Scaling {SGD} {Batch} {Size} to {32K} for {ImageNet} {Training}},
	volume = {abs/1708.03888},
	journal = {CoRR},
	author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	year = {2017},
	note = {\_eprint: 1708.03888}
}

@inproceedings{wu_how_2018-1,
	title = {How {SGD} {Selects} the {Global} {Minima} in {Over}-parameterized {Learning}: {A} {Dynamical} {Stability} {Perspective}},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Wu, Lei and Ma, Chao and E., Weinan},
	year = {2018}
}

@inproceedings{zhang_three_2019-1,
	title = {Three {Mechanisms} of {Weight} {Decay} {Regularization}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger},
	year = {2019}
}

@article{arnold_reducing_2019,
	title = {Reducing the variance in online optimization by transporting past gradients},
	volume = {abs/1906.03532},
	journal = {CoRR},
	author = {Arnold, Sébastien M. R. and Manzagol, Pierre-Antoine and Babanezhad, Reza and Mitliagkas, Ioannis and Roux, Nicolas Le},
	year = {2019},
	note = {\_eprint: 1906.03532}
}

@incollection{johnson_accelerating_2013,
	title = {Accelerating {Stochastic} {Gradient} {Descent} using {Predictive} {Variance} {Reduction}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	author = {Johnson, Rie and Zhang, Tong},
	year = {2013}
}

@inproceedings{pascanu_difficulty_2013,
	title = {On the difficulty of training recurrent neural networks},
	booktitle = {International conference on machine learning},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	year = {2013}
}

@article{thomas_information_2019,
	title = {Information matrices and generalization},
	volume = {abs/1906.07774},
	journal = {CoRR},
	author = {Thomas, Valentin and Pedregosa, Fabian and Merriënboer, Bart van and Manzagol, Pierre-Antoine and Bengio, Yoshua and Roux, Nicolas Le},
	year = {2019},
	note = {\_eprint: 1906.07774}
}

@incollection{zhang_which_2019,
	title = {Which {Algorithmic} {Choices} {Matter} at {Which} {Batch} {Sizes}? {Insights} {From} a {Noisy} {Quadratic} {Model}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George and Shallue, Chris and Grosse, Roger B},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8196--8207}
}

@inproceedings{liu_understanding_2020,
	title = {Understanding {Why} {Neural} {Networks} {Generalize} {Well} {Through} {GSNR} of {Parameters}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Liu, Jinlong and Bai, Yunzhi and Jiang, Guoqing and Chen, Ting and Wang, Huayan},
	year = {2020}
}

@inproceedings{he_local_2020,
	title = {The {Local} {Elasticity} of {Neural} {Networks}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {He, Hangfeng and Su, Weijie},
	year = {2020}
}

@incollection{li_towards_2019,
	title = {Towards {Explaining} the {Regularization} {Effect} of {Initial} {Large} {Learning} {Rate} in {Training} {Neural} {Networks}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
	year = {2019}
}

@article{carbonnelle_layer-level_2018-1,
	title = {On layer-level control of {DNN} training and its impact on generalization},
	volume = {abs/1806.01603},
	journal = {CoRR},
	author = {Carbonnelle, Simon and Vleeschouwer, Christophe De},
	year = {2018}
}

@inproceedings{luo_towards_2019,
	title = {Towards {Understanding} {Regularization} in {Batch} {Normalization}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Luo, Ping and Wang, Xinjiang and Shao, Wenqi and Peng, Zhanglin},
	year = {2019}
}

@inproceedings{fort_goldilocks_2019,
	title = {The goldilocks zone: {Towards} better understanding of neural network loss landscapes},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Fort, Stanislav and Scherlis, Adam},
	year = {2019}
}

@article{fort_stiffness_2019,
	title = {Stiffness: {A} {New} {Perspective} on {Generalization} in {Neural} {Networks}},
	journal = {arXiv preprint arXiv:1901.09491},
	author = {Fort, Stanislav and Nowak, Paweł Krzysztof and Jastrzębski, Stanisław and Narayanan, Srini},
	year = {2019},
	note = {\_eprint: 1901.09491}
}

@article{fort_large_2019,
	title = {Large {Scale} {Structure} of {Neural} {Network} {Loss} {Landscapes}},
	journal = {arXiv preprint arXiv:1906.04724},
	author = {Fort, Stanislav and Jastrzebski, Stanislaw},
	year = {2019},
	note = {\_eprint: 1906.04724}
}

@incollection{lecun_automatic_1993,
	title = {Automatic {Learning} {Rate} {Maximization} by {On}-{Line} {Estimation} of the {Hessian}{\textbackslash}textquotesingle s {Eigenvectors}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 5},
	publisher = {Morgan-Kaufmann},
	author = {LeCun, Yann and Simard, Patrice Y. and Pearlmutter, Barak},
	editor = {Hanson, S. J. and Cowan, J. D. and Giles, C. L.},
	year = {1993}
}

@article{you_scaling_2017-2,
	title = {Scaling {SGD} {Batch} {Size} to {32K} for {ImageNet} {Training}},
	volume = {abs/1708.03888},
	journal = {CoRR},
	author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	year = {2017}
}

@article{alain_negative_2019-1,
	title = {Negative eigenvalues of the {Hessian} in deep neural networks},
	volume = {abs/1902.02366},
	journal = {CoRR},
	author = {Alain, Guillaume and Roux, Nicolas Le and Manzagol, Pierre-Antoine},
	year = {2019},
	note = {\_eprint: 1902.02366}
}

@book{goodfellow_deep_2016-1,
	title = {Deep {Learning}},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016}
}

@article{gur-ari_gradient_2018-1,
	title = {Gradient {Descent} {Happens} in a {Tiny} {Subspace}},
	journal = {arXiv e-prints},
	author = {Gur-Ari, Guy and Roberts, Daniel A. and Dyer, Ethan},
	month = dec,
	year = {2018},
	note = {\_eprint: 1812.04754},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{ghorbani_investigation_2019,
	title = {An {Investigation} into {Neural} {Net} {Optimization} via {Hessian} {Eigenvalue} {Density}},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
	year = {2019}
}

@article{lanczos_iteration_1950,
	title = {An iteration method for the solution of the eigenvalue problem of linear differential and integral operators},
	journal = {J. Res. Natl. Bur. Stand. B},
	author = {Lanczos, Cornelius},
	year = {1950}
}

@article{lan_lca_2019,
	title = {{LCA}: {Loss} {Change} {Allocation} for {Neural} {Network} {Training}},
	journal = {arXiv preprint arXiv:1909.01440},
	author = {Lan, Janice and Liu, Rosanne and Zhou, Hattie and Yosinski, Jason},
	year = {2019}
}

@inproceedings{jiang_fantastic_2020,
	title = {Fantastic {Generalization} {Measures} and {Where} to {Find} {Them}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jiang, Yiding and Neyshabur, Behnam and Krishnan, Dilip and Mobahi, Hossein and Bengio, Samy},
	year = {2020}
}

@incollection{lecun_efficient_2012,
	title = {Efficient {BackProp}.},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade} (2nd ed.)},
	author = {LeCun, Yann and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
	year = {2012}
}

@article{sagun_empirical_2017-1,
	title = {Empirical {Analysis} of the {Hessian} of {Over}-{Parametrized} {Neural} {Networks}},
	volume = {abs/1706.04454},
	journal = {CoRR},
	author = {Sagun, Levent and Evci, Utku and Güney, V. Ugur and Dauphin, Yann N. and Bottou, Léon},
	year = {2017},
	note = {\_eprint: 1706.04454}
}

@article{xiao_fashion-mnist_2017,
	title = {Fashion-{MNIST}: a {Novel} {Image} {Dataset} for {Benchmarking} {Machine} {Learning} {Algorithms}},
	volume = {abs/1708.07747},
	journal = {CoRR},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	year = {2017},
	note = {\_eprint: 1708.07747}
}

@book{he_local_2019,
	title = {The {Local} {Elasticity} of {Neural} {Networks}},
	author = {He, Hangfeng and Su, Weijie J.},
	year = {2019},
	note = {\_eprint: 1910.06943}
}

@article{devlin_bert_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	journal = {arXiv preprint arXiv:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018}
}

@article{dauphin_identifying_2014,
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	volume = {abs/1406.2572},
	journal = {CoRR},
	author = {Dauphin, Yann N. and Pascanu, Razvan and Gülçehre, Çaglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	year = {2014},
	note = {\_eprint: 1406.2572}
}

@article{yang_mean_2019,
	title = {A {Mean} {Field} {Theory} of {Batch} {Normalization}},
	volume = {abs/1902.08129},
	journal = {CoRR},
	author = {Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.},
	year = {2019},
	note = {\_eprint: 1902.08129}
}

@inproceedings{netzer_reading_2011,
	title = {Reading {Digits} in {Natural} {Images} with {Unsupervised} {Feature} {Learning}},
	booktitle = {{NIPS} {Workshop} on {Deep} {Learning} and {Unsupervised} {Feature} {Learning} 2011},
	author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y.},
	year = {2011}
}

@article{erhan_why_2010,
	title = {Why {Does} {Unsupervised} {Pre}-training {Help} {Deep} {Learning}?},
	journal = {J. Mach. Learn. Res.},
	author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
	year = {2010}
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} {Large}-{Scale} {Hierarchical} {Image} {Database}},
	booktitle = {{CVPR09}},
	author = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
	year = {2009}
}

@book{chollet_keras_2015,
	title = {Keras},
	publisher = {GitHub},
	author = {Chollet, François and {others}},
	year = {2015}
}

@article{huang_densely_2016,
	title = {Densely {Connected} {Convolutional} {Networks}},
	volume = {abs/1608.06993},
	journal = {CoRR},
	author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q.},
	year = {2016},
	note = {\_eprint: 1608.06993}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015}
}

@inproceedings{maas_learning_2011,
	title = {Learning {Word} {Vectors} for {Sentiment} {Analysis}},
	booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	year = {2011}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	journal = {Neural Comput.},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997}
}

@article{wattenberg_how_2016,
	title = {How to {Use} t-{SNE} {Effectively}},
	journal = {Distill},
	author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
	year = {2016}
}

@incollection{noauthor_dynamic_2007,
	address = {Berlin, Heidelberg},
	title = {Dynamic {Time} {Warping}},
	isbn = {978-3-540-74048-3},
	abstract = {Dynamic time warping (DTW) is a well-known technique to find an optimal alignment between two given (time-dependent) sequences under certain restrictions (Fig. 4.1). Intuitively, the sequences are warped in a nonlinear fashion to match each other. Originally, DTW has been used to compare different speech patterns in automatic speech recognition, see [170]. In fields such as data mining and information retrieval, DTW has been successfully applied to automatically cope with time deformations and different speeds associated with time-dependent data.},
	booktitle = {Information {Retrieval} for {Music} and {Motion}},
	publisher = {Springer Berlin Heidelberg},
	year = {2007},
	pages = {69--84}
}

@article{mcinnes_umap_2018,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection}},
	journal = {The Journal of Open Source Software},
	author = {McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},
	year = {2018}
}

@article{gur-ari_gradient_2018-2,
	title = {Gradient {Descent} {Happens} in a {Tiny} {Subspace}},
	volume = {abs/1812.04754},
	journal = {CoRR},
	author = {Gur-Ari, Guy and Roberts, Daniel A. and Dyer, Ethan},
	year = {2018},
	note = {\_eprint: 1812.04754}
}

@incollection{johnson_accelerating_2013-1,
	title = {Accelerating {Stochastic} {Gradient} {Descent} using {Predictive} {Variance} {Reduction}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Johnson, Rie and Zhang, Tong},
	year = {2013}
}

@article{yao_hessian-based_2018-1,
	title = {Hessian-based {Analysis} of {Large} {Batch} {Training} and {Robustness} to {Adversaries}},
	volume = {abs/1802.08241},
	journal = {CoRR},
	author = {Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2018}
}

@incollection{lecun_efficient_2012-1,
	address = {Berlin, Heidelberg},
	title = {Efficient {BackProp}},
	isbn = {978-3-642-35289-8},
	abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}: {Second} {Edition}},
	publisher = {Springer Berlin Heidelberg},
	author = {LeCun, Yann A. and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_3},
	pages = {9--48}
}

@book{martens_second-order_2016-1,
	title = {Second-order optimization for neural networks},
	publisher = {University of Toronto (Canada)},
	author = {Martens, James},
	year = {2016}
}

@article{martens_optimizing_2015-1,
	title = {Optimizing {Neural} {Networks} with {Kronecker}-factored {Approximate} {Curvature}},
	volume = {abs/1503.05671},
	journal = {CoRR},
	author = {Martens, James and Grosse, Roger B.},
	year = {2015}
}

@article{zhu_anisotropic_2018-1,
	title = {The {Anisotropic} {Noise} in {Stochastic} {Gradient} {Descent}: {Its} {Behavior} of {Escaping} from {Minima} and {Regularization} {Effects}},
	journal = {arXiv preprint arXiv:1803.00195},
	author = {Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
	year = {2018}
}

@article{jastrzebski_three_2017-1,
	title = {Three {Factors} {Influencing} {Minima} in {SGD}},
	volume = {abs/1711.04623},
	journal = {CoRR},
	author = {Jastrzebski, Stanislaw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos J.},
	year = {2017}
}

@article{xing_walk_2018-1,
	title = {A {Walk} with {SGD}},
	journal = {arXiv preprint arXiv:1802.08770},
	author = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
	year = {2018}
}

@article{goodfellow_qualitatively_2014-1,
	title = {Qualitatively characterizing neural network optimization problems},
	journal = {arXiv preprint arXiv:1412.6544},
	author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
	year = {2014},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{goyal_accurate_2017-1,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	volume = {abs/1706.02677},
	journal = {CoRR},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross B. and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	year = {2017}
}

@article{kingma_adam_2014-1,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	journal = {arXiv e-prints},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {\_eprint: 1412.6980},
	keywords = {Computer Science - Machine Learning}
}

@article{masters_revisiting_2018-1,
	title = {Revisiting {Small} {Batch} {Training} for {Deep} {Neural} {Networks}},
	volume = {abs/1804.07612},
	journal = {CoRR},
	author = {Masters, Dominic and Luschi, Carlo},
	year = {2018}
}

@article{hoffer_train_2017-1,
	title = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	journal = {arXiv e-prints},
	author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	month = may,
	year = {2017},
	note = {\_eprint: 1705.08741},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	author = {Krizhevsky, Alex},
	year = {2009}
}

@unpublished{bietti_inductive_2019,
	title = {On the {Inductive} {Bias} of {Neural} {Tangent} {Kernels}},
	author = {Bietti, Alberto and Mairal, Julien},
	month = may,
	year = {2019},
	annote = {working paper or preprint}
}

@inproceedings{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	booktitle = {Proceedings of the {32Nd} {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015}
}

@article{page_how_2019,
	title = {How to {Train} {Your} {ResNet} 7: {Batch} {Norm}},
	author = {Page, David},
	year = {2019}
}

@article{sagun_empirical_2017-2,
	title = {Empirical {Analysis} of the {Hessian} of {Over}-{Parametrized} {Neural} {Networks}},
	volume = {abs/1706.04454},
	journal = {CoRR},
	author = {Sagun, Levent and Evci, Utku and Güney, V. Ugur and Dauphin, Yann and Bottou, Léon},
	year = {2017}
}

@inproceedings{dinh_sharp_2017-1,
	address = {International Convention Centre, Sydney, Australia},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Sharp {Minima} {Can} {Generalize} {For} {Deep} {Nets}},
	volume = {70},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017}
}

@article{papyan_measurements_2019-1,
	title = {Measurements of {Three}-{Level} {Hierarchical} {Structure} in the {Outliers} in the {Spectrum} of {Deepnet} {Hessians}},
	volume = {abs/1901.08244},
	journal = {CoRR},
	author = {Papyan, Vardan},
	year = {2019}
}

@article{novak_sensitivity_2018-1,
	title = {Sensitivity and {Generalization} in {Neural} {Networks}: an {Empirical} {Study}},
	journal = {arXiv e-prints},
	author = {Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	month = feb,
	year = {2018},
	note = {\_eprint: 1802.08760},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{jastrzebski_relation_2018-1,
	title = {On the {Relation} {Between} the {Sharpest} {Directions} of {DNN} {Loss} and the {SGD} {Step} {Length}},
	journal = {arXiv preprint arXiv: 1807.0531},
	author = {Jastrzebski, Stanislaw and Kenton, Zachary and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
	year = {2018}
}

@article{golatkar_time_2019,
	title = {Time {Matters} in {Regularizing} {Deep} {Networks}: {Weight} {Decay} and {Data} {Augmentation} {Affect} {Early} {Learning} {Dynamics}, {Matter} {Little} {Near} {Convergence}},
	journal = {arXiv preprint arXiv:1905.13277},
	author = {Golatkar, Aditya and Achille, Alessandro and Soatto, Stefano},
	year = {2019}
}

@inproceedings{li_stochastic_2017,
	address = {International Convention Centre, Sydney, Australia},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Stochastic {Modified} {Equations} and {Adaptive} {Stochastic} {Gradient} {Algorithms}},
	volume = {70},
	abstract = {We develop the method of stochastic modified equations (SME), in which stochastic gradient algorithms are approximated in the weak sense by continuous-time stochastic differential equations. We exploit the continuous formulation together with optimal control theory to derive novel adaptive hyper-parameter adjustment policies. Our algorithms have competitive performance with the added benefit of being robust to varying models and datasets. This provides a general methodology for the analysis and design of stochastic gradient algorithms.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	pages = {2101--2110}
}

@incollection{bartlett_spectrally-normalized_2017,
	title = {Spectrally-normalized margin bounds for neural networks},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
	year = {2017}
}

@inproceedings{arora_convergence_2019,
	title = {A {Convergence} {Analysis} of {Gradient} {Descent} for {Deep} {Linear} {Neural} {Networks}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
	year = {2019}
}

@article{achille_critical_2017-1,
	title = {Critical {Learning} {Periods} in {Deep} {Neural} {Networks}},
	volume = {abs/1711.08856},
	journal = {CoRR},
	author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
	year = {2017}
}

@incollection{bjorck_understanding_2018,
	title = {Understanding {Batch} {Normalization}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bjorck, Nils and Gomes, Carla P and Selman, Bart and Weinberger, Kilian Q},
	year = {2018}
}

@incollection{roux_topmoumoute_2008,
	title = {Topmoumoute {Online} {Natural} {Gradient} {Algorithm}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Roux, Nicolas L. and Manzagol, Pierre-antoine and Bengio, Yoshua},
	year = {2008}
}

@article{neyshabur_implicit_2017,
	title = {Implicit {Regularization} in {Deep} {Learning}},
	volume = {abs/1709.01953},
	journal = {CoRR},
	author = {Neyshabur, Behnam},
	year = {2017},
	note = {\_eprint: 1709.01953}
}

@inproceedings{keskar_large-batch_2017,
	title = {On {Large}-{Batch} {Training} for {Deep} {Learning}: {Generalization} {Gap} and {Sharp} {Minima}},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}, {ICLR}},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	year = {2017}
}

@article{achille_critical_2017-2,
	title = {Critical {Learning} {Periods} in {Deep} {Neural} {Networks}},
	volume = {abs/1711.08856},
	journal = {CoRR},
	author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
	year = {2017}
}

@article{arpit_closer_2017-1,
	title = {A {Closer} {Look} at {Memorization} in {Deep} {Networks}},
	journal = {arXiv e-prints},
	author = {Arpit, Devansh and Jastrzębski, Stanis{\textbackslash}law and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
	month = jun,
	year = {2017},
	note = {\_eprint: 1706.05394},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{huang_densely_2016-1,
	title = {Densely {Connected} {Convolutional} {Networks}},
	volume = {abs/1608.06993},
	journal = {CoRR},
	author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q.},
	year = {2016},
	note = {\_eprint: 1608.06993}
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	journal = {International Journal of Computer Vision (IJCV)},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	year = {2015}
}

@inproceedings{williams_broad-coverage_2018,
	title = {A {Broad}-{Coverage} {Challenge} {Corpus} for {Sentence} {Understanding} through {Inference}},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Williams, Adina and Nangia, Nikita and Bowman, Samuel},
	year = {2018}
}
